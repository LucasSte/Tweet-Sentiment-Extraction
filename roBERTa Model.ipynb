{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet sentiment extratcion\n",
    "#### Training and inference model based upon roBERTa\n",
    "\n",
    "Inspired by [this](https://www.kaggle.com/cdeotte/tensorflow-roberta-0-705) Kaggle notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries\n",
    "* **Pandas** and **NumPy** for computational mathematics\n",
    "* **Tensorflow** for machine learning\n",
    "* **Sklearn** (StratfiedKFold) for spliting the data into balanced distributions\n",
    "* **transformers** (from Hunggingface) NPL library for tensorflow 2.0\n",
    "* **tokenizers** (from Huggingface) implementation of modern tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializer tokenizer\n",
    "A tokenizer is an algorithm that transform words into symbols that a neural network can understand.\n",
    "\n",
    "**ByteLevelBPETokenizer**\n",
    "BPE (Byte-Pair Econding) tokenizer has a vocabulary that consists of single letters and sets of letters. When we create a vocabulary for this tokenizer, we start with all the letters as tokens and we merge tokens whose juxtaposition is frequent on the data set. However, if we consider UTF-8 charecters, the dictionary might get too big. To optimize our tokenizer, instead of working with letters as tokens, we use bytes as tokens.\n",
    "This function requires two files as arguments. ```merges``` contains all the merged tokens and ```vocab``` contains pairs (key, value), in which keys are tokens and values are numbers as input for the neural network.\n",
    "\n",
    "For this experiment, the files used here are available in the [Huggingface website](https://huggingface.co/roberta-base/tree/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab= path + '/vocab.json',\n",
    "    merges = path + '/merges.txt',\n",
    "    lowercase = True, #All tokens are in lower case\n",
    "    add_prefix_space=True #Do not treat spaces like part of the tokens\n",
    ")\n",
    "\n",
    "#Get the ids to decode the neural network output\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974} \n",
    "\n",
    "train_set = pd.read_csv(path+'/train.csv').fillna('')\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by initializing some auxiliar variables to parse the tweets.\n",
    "\n",
    "* **max_lenght**: the maximum size of a tokenized tweet.\n",
    "* **ct**: saves the size of the training set.\n",
    "* **input_ids**: saves the IDs for each token the algorithm detected.\n",
    "* **attention_mask**: As tweets have different sizes, attention_mask signals the token IDs that should be read by the model.\n",
    "* **start_token**: Using a one-hot notation, saves the position where the selected fragment starts.\n",
    "* **end_token**: Using a one-hot notation, saves the position where the selected fragment ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 96\n",
    "ct = train_set.shape[0]\n",
    "input_ids = np.ones((ct, max_length), dtype='int32')\n",
    "attention_mask = np.zeros((ct, max_length), dtype='int32')\n",
    "start_tokens = np.zeros((ct, max_length), dtype='int32')\n",
    "end_tokens = np.zeros((ct, max_length), dtype='int32')\n",
    "\n",
    "for i in range(ct):\n",
    "    \n",
    "    #Find where selected text sits inside the tweet\n",
    "    text1 = \" \"+\" \".join(train_set.loc[i, 'text'].split())\n",
    "    text2 = \" \".join(train_set.loc[i, 'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)] = 1\n",
    "    if text1[idx-1] == ' ':\n",
    "        chars[idx-1] = 1\n",
    "        \n",
    "    #Encode the text and find the selected_text offset, as the\n",
    "    #encoded vector might not have the same length as the text vector\n",
    "    enc = tokenizer.encode(text1)\n",
    "    offsets = []\n",
    "    idx = 0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx, idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    #Find the ids of the selected_text in the tokenized text\n",
    "    tokens = []\n",
    "    for k, (a, b) in enumerate(offsets):\n",
    "        s = np.sum(chars[a:b])\n",
    "        if s > 0:\n",
    "            tokens.append(k)\n",
    "    \n",
    "    #After precessing, fill the vectors\n",
    "    sent_token = sentiment_id[train_set.loc[i, 'sentiment']]\n",
    "    input_ids[i, :len(enc.ids)+5] = [0] + enc.ids + [2,2] + [sent_token] + [2]\n",
    "    attention_mask[i, :len(enc.ids)+5] = 1\n",
    "    \n",
    "    if len(tokens) > 0:\n",
    "        start_tokens[i, tokens[0]+1]=1\n",
    "        end_tokens[i, tokens[-1]+1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading testing set\n",
    "\n",
    "We must enconde our testing set the same way we tokenized our training set. Variables here have analogous names to those of the last case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv(path+'/test.csv').fillna('')\n",
    "\n",
    "ct = test_set.shape[0]\n",
    "input_ids_test = np.ones((ct, max_length), dtype='int32')\n",
    "attention_mask_test = np.zeros((ct, max_length), dtype='int32')\n",
    "\n",
    "#We do not need to find the selected text for the testing set for\n",
    "# the algorithm will detect and it will be compared with the fragment\n",
    "# on the csv file\n",
    "for i in range(ct):\n",
    "    \n",
    "    text1 = \" \" + \" \".join(test_set.loc[i, 'text'].split())\n",
    "    enc = tokenizer.encode(text1)\n",
    "    sentiment_token = sentiment_id[test_set.loc[i, 'sentiment']]\n",
    "    input_ids_test[k, :len(enc.ids)+5] = [0] + enc.ids + [2,2] + [sentiment_token] + [2]\n",
    "    attention_mask_test[k, :len(enc.ids)+5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
