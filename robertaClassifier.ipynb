{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet sentiment extratcion\n",
    "#### Training and inference model based upon roBERTa\n",
    "\n",
    "Inspired by [this](https://www.kaggle.com/cdeotte/tensorflow-roberta-0-705) Kaggle notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries\n",
    "* **Pandas** and **NumPy** for computational mathematics\n",
    "* **Tensorflow** for machine learning\n",
    "* **Sklearn** (StratfiedKFold) for spliting the data into balanced distributions\n",
    "* **transformers** (from Hunggingface) NPL library for tensorflow 2.0\n",
    "* **tokenizers** (from Huggingface) implementation of modern tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bjm_o3sRKTJG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Bhy59-y3H2LO"
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializer tokenizer\n",
    "A tokenizer is an algorithm that transform words into symbols that a neural network can understand.\n",
    "\n",
    "**ByteLevelBPETokenizer**\n",
    "BPE (Byte-Pair Econding) tokenizer has a vocabulary that consists of single letters and sets of letters. When we create a vocabulary for this tokenizer, we start with all the letters as tokens and we merge tokens whose juxtaposition is frequent on the data set. However, if we consider UTF-8 charecters, the dictionary might get too big. To optimize our tokenizer, instead of working with letters as tokens, we use bytes as tokens.\n",
    "This function requires two files as arguments. ```merges``` contains all the merged tokens and ```vocab``` contains pairs (key, value), in which keys are tokens and values are numbers as input for the neural network.\n",
    "\n",
    "For this experiment, the files used here are available in the [Huggingface website](https://huggingface.co/roberta-base/tree/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "q4shRMxvINHa",
    "outputId": "0fc99dcd-2df8-4249-bd75-37cd553ca4f4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab= path + '/vocab.json',\n",
    "    merges = path + '/merges.txt',\n",
    "    lowercase = True, #All tokens are in lower case\n",
    "    add_prefix_space=True #Do not treat spaces like part of the tokens\n",
    ")\n",
    "\n",
    "#Get the ids to decode the neural network output\n",
    "sentiment_id = {'positive': [1, 0, 0], 'negative': [0, 0, 1], 'neutral': [0, 1, 0]} \n",
    "\n",
    "train_set = pd.read_csv(path+'/train.csv').fillna('')\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3BY0cCK4NEm"
   },
   "source": [
    "We start by initializing some auxiliar variables to parse the tweets.\n",
    "\n",
    "* **max_lenght**: the maximum size of a tokenized tweet.\n",
    "* **ct**: saves the size of the training set.\n",
    "* **input_ids**: saves the IDs for each token the algorithm detected. Token IDs are numerical representations of tokens building sequences.\n",
    "* **attention_mask**: As tweets have different sizes, attention_mask signals the token IDs that should be read by the model. It indicates to the model which tokens should be attended to, and which should not.\n",
    "* **token_type_ids** Some models' purpose it to do questions answering. With roBERTa, token type IDs identify which section of the phrase is a question and which is an answer. As this is not our goal, we leave it zero.\n",
    "* **sent_class**: Type of sentiment: positive, negative or neutral. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Tbu-p6FRJFTX"
   },
   "outputs": [],
   "source": [
    "max_length = 96\n",
    "ct = train_set.shape[0]\n",
    "input_ids = np.ones((ct, max_length), dtype='int32')\n",
    "attention_mask = np.zeros((ct, max_length), dtype='int32')\n",
    "token_type_ids = np.zeros((ct, max_length),dtype='int32')\n",
    "sent_class = np.zeros((ct, 3), dtype='int32')\n",
    "\n",
    "for i in range(ct):\n",
    "    \n",
    "    #Find where selected text sits inside the tweet\n",
    "    text1 = \" \"+\" \".join(train_set.loc[i, 'text'].split())\n",
    "        \n",
    "    #Encode the text and find the selected_text offset, as the\n",
    "    #encoded vector might not have the same length as the text vector\n",
    "    enc = tokenizer.encode(text1)\n",
    "    \n",
    "    #After precessing, fill the vectors\n",
    "    input_ids[i, :len(enc.ids)+2] = [0] + enc.ids + [2]\n",
    "    attention_mask[i, :len(enc.ids)+2] = 1\n",
    "    sent_class[i, :] = sentiment_id[train_set.loc[i, 'sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oe-aZf-94hDI"
   },
   "source": [
    "### Loading testing set\n",
    "\n",
    "We must enconde our testing set the same way we tokenized our training set. Variables here have analogous names to those of the last case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MLHH6TMHJhJV"
   },
   "outputs": [],
   "source": [
    "test_set = pd.read_csv(path+'/test.csv').fillna('')\n",
    "\n",
    "ct = test_set.shape[0]\n",
    "input_ids_test = np.ones((ct, max_length), dtype='int32')\n",
    "attention_mask_test = np.zeros((ct, max_length), dtype='int32')\n",
    "token_type_ids_test = np.zeros((ct, max_length),dtype='int32')\n",
    "sent_class_test = np.zeros((ct, 3), dtype='int32')\n",
    "\n",
    "#We do not need to find the selected text for the testing set for\n",
    "# the algorithm will detect and it will be compared with the fragment\n",
    "# on the csv file\n",
    "for k in range(ct):\n",
    "    text1 = \" \" + \" \".join(test_set.loc[k, 'text'].split())\n",
    "    enc = tokenizer.encode(text1)\n",
    "    input_ids_test[k, :len(enc.ids)+2] = [0] + enc.ids + [2]\n",
    "    sent_class_test[k, :] = sentiment_id[test_set.loc[k, 'sentiment']]\n",
    "    attention_mask_test[k, :len(enc.ids)+2] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMnyqxSwvAOA"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IBItxGr_JnmB"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((max_length,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((max_length,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((max_length,), dtype=tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(path+'/config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(path+'/pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "        \n",
    "    x3 = tf.keras.layers.Dropout(0.1)(x[0])\n",
    "    x3 = tf.keras.layers.Conv1D(8, 2, padding='same')(x3)\n",
    "    x3 = tf.keras.layers.LeakyReLU()(x3)\n",
    "    x3 = tf.keras.layers.Conv1D(4, 2, padding='same')(x3)\n",
    "    x3 = tf.keras.layers.Flatten()(x3)\n",
    "    x3 = tf.keras.layers.Dense(8, activation='relu')(x3)\n",
    "    x3 = tf.keras.layers.Dense(3, activation='relu')(x3)\n",
    "    x3 = tf.keras.layers.Activation('softmax')(x3)\n",
    "    \n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x3])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddEc_QiGvF2E"
   },
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pWxHf15TEtv4",
    "outputId": "cae14588-fc41-4f02-e12a-30eaf71837a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at /home/lucas/Documents/PO-240/sentiment-extraction/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 96)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 96)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 96)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode TFBaseModelOutputWit 124645632   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 96, 768)      0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 96, 8)        12296       dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 96, 8)        0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 96, 4)        68          leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 384)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 8)            3080        flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            27          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 3)            0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 124,661,103\n",
      "Trainable params: 124,661,103\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ver_model = build_model()\n",
    "ver_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6JUqXwGvLXK"
   },
   "source": [
    "## Training and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Os-AN0r2JrLE",
    "outputId": "a3fa7afe-4b11-4e88-a2aa-40d68c05e430"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at /home/lucas/Documents/PO-240/sentiment-extraction/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[32,96,3072] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node functional_1/tf_roberta_model/roberta/encoder/layer_._4/intermediate/activation/mul_1 (defined at /home/lucas/anaconda3/envs/iczin/lib/python3.8/site-packages/transformers/activations_tf.py:16) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[gradient_tape/functional_1/tf_roberta_model/roberta/embeddings/token_type_embeddings/embedding_lookup/Reshape/_46]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[32,96,3072] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node functional_1/tf_roberta_model/roberta/encoder/layer_._4/intermediate/activation/mul_1 (defined at /home/lucas/anaconda3/envs/iczin/lib/python3.8/site-packages/transformers/activations_tf.py:16) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_30699]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node functional_1/tf_roberta_model/roberta/encoder/layer_._4/intermediate/activation/mul_1:\n functional_1/tf_roberta_model/roberta/encoder/layer_._4/intermediate/activation/mul (defined at /home/lucas/anaconda3/envs/iczin/lib/python3.8/site-packages/transformers/activations_tf.py:14)\t\n functional_1/tf_roberta_model/roberta/encoder/layer_._4/intermediate/dense/BiasAdd (defined at /home/lucas/anaconda3/envs/iczin/lib/python3.8/site-packages/transformers/models/roberta/modeling_tf_roberta.py:374)\n\nInput Source operations connected to node functional_1/tf_roberta_model/roberta/encoder/layer_._4/intermediate/activation/mul_1:\n functional_1/tf_roberta_model/roberta/encoder/layer_._4/intermediate/activation/mul (defined at /home/lucas/anaconda3/envs/iczin/lib/python3.8/site-packages/transformers/activations_tf.py:14)\t\n functional_1/tf_roberta_model/roberta/encoder/layer_._4/intermediate/dense/BiasAdd (defined at /home/lucas/anaconda3/envs/iczin/lib/python3.8/site-packages/transformers/models/roberta/modeling_tf_roberta.py:374)\n\nFunction call stack:\ntrain_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-eeeb7f30515e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         save_weights_only=True, mode='auto', save_freq='epoch')\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [sent_class[idxT,]], \n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDISPLAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[32,96,3072] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node functional_1/tf_roberta_model/roberta/encoder/layer_._4/intermediate/activation/mul_1 (defined at /home/lucas/anaconda3/envs/iczin/lib/python3.8/site-packages/transformers/activations_tf.py:16) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[gradient_tape/functional_1/tf_roberta_model/roberta/embeddings/token_type_embeddings/embedding_lookup/Reshape/_46]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[32,96,3072] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node functional_1/tf_roberta_model/roberta/encoder/layer_._4/intermediate/activation/mul_1 (defined at /home/lucas/anaconda3/envs/iczin/lib/python3.8/site-packages/transformers/activations_tf.py:16) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_30699]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node functional_1/tf_roberta_model/roberta/encoder/layer_._4/intermediate/activation/mul_1:\n functional_1/tf_roberta_model/roberta/encoder/layer_._4/intermediate/activation/mul (defined at /home/lucas/anaconda3/envs/iczin/lib/python3.8/site-packages/transformers/activations_tf.py:14)\t\n functional_1/tf_roberta_model/roberta/encoder/layer_._4/intermediate/dense/BiasAdd (defined at /home/lucas/anaconda3/envs/iczin/lib/python3.8/site-packages/transformers/models/roberta/modeling_tf_roberta.py:374)\n\nInput Source operations connected to node functional_1/tf_roberta_model/roberta/encoder/layer_._4/intermediate/activation/mul_1:\n functional_1/tf_roberta_model/roberta/encoder/layer_._4/intermediate/activation/mul (defined at /home/lucas/anaconda3/envs/iczin/lib/python3.8/site-packages/transformers/activations_tf.py:14)\t\n functional_1/tf_roberta_model/roberta/encoder/layer_._4/intermediate/dense/BiasAdd (defined at /home/lucas/anaconda3/envs/iczin/lib/python3.8/site-packages/transformers/models/roberta/modeling_tf_roberta.py:374)\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "jac = []; VER='v1'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "oof_class = np.zeros((input_ids.shape[0], 3))\n",
    "ct = test_set.shape[0]\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train_set.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "        \n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        '%s-roberta-ext-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "\n",
    "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [sent_class[idxT,]], \n",
    "        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n",
    "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
    "        [sent_class[idxV,]]))\n",
    "    \n",
    "    print('Loading model...')\n",
    "    model.load_weights('%s-roberta-ext-%i.h5'%(VER,fold))\n",
    "    \n",
    "\n",
    "    print('Predicting OOF...')\n",
    "    oof_class[idxV, ] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    print('Predicting test ...')\n",
    "    preds = model.predict([input_ids_test,attention_mask_test,token_type_ids_test],verbose=DISPLAY)\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    right = 0\n",
    "    wrong = 0\n",
    "    for k in idxV:\n",
    "        c = np.argmax(oof_class[k, ])\n",
    "        d = np.argmax(sent_class[k, ])\n",
    "        if c == d:\n",
    "            right += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "    ans = float(right)/float(right + wrong)\n",
    "    print('Validation right =', ans)\n",
    "    right = 0\n",
    "    wrong = 0\n",
    "    for i in range(ct):\n",
    "        a = np.argmax(sent_class_test[i, ])\n",
    "        b = np.argmax(preds[i, ])\n",
    "        if a == b:\n",
    "            right += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "            \n",
    "    ans = float(right)/float(right + wrong)\n",
    "    print('Testing set right = ', ans)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "robertaExtended.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
