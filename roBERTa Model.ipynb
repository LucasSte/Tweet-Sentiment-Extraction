{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet sentiment extratcion\n",
    "#### Training and inference model based upon roBERTa\n",
    "\n",
    "Inspired by [this](https://www.kaggle.com/cdeotte/tensorflow-roberta-0-705) Kaggle notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries\n",
    "* **Pandas** and **NumPy** for computational mathematics\n",
    "* **Tensorflow** for machine learning\n",
    "* **Sklearn** (StratfiedKFold) for spliting the data into balanced distributions\n",
    "* **transformers** (from Hunggingface) NPL library for tensorflow 2.0\n",
    "* **tokenizers** (from Huggingface) implementation of modern tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializer tokenizer\n",
    "A tokenizer is an algorithm that transform words into symbols that a neural network can understand.\n",
    "\n",
    "**ByteLevelBPETokenizer**\n",
    "BPE (Byte-Pair Econding) tokenizer has a vocabulary that consists of single letters and sets of letters. When we create a vocabulary for this tokenizer, we start with all the letters as tokens and we merge tokens whose juxtaposition is frequent on the data set. However, if we consider UTF-8 charecters, the dictionary might get too big. To optimize our tokenizer, instead of working with letters as tokens, we use bytes as tokens.\n",
    "This function requires two files as arguments. ```merges``` contains all the merged tokens and ```vocab``` contains pairs (key, value), in which keys are tokens and values are numbers as input for the neural network.\n",
    "\n",
    "For this experiment, the files used here are available in the [Huggingface website](https://huggingface.co/roberta-base/tree/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab= path + '/vocab.json',\n",
    "    merges = path + '/merges.txt',\n",
    "    lowercase = True, #All tokens are in lower case\n",
    "    add_prefix_space=True #Do not treat spaces like part of the tokens\n",
    ")\n",
    "\n",
    "#Get the ids to decode the neural network output\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974} \n",
    "\n",
    "train_set = pd.read_csv(path+'/train.csv').fillna('')\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by initializing some auxiliar variables to parse the tweets.\n",
    "\n",
    "* **max_lenght**: the maximum size of a tokenized tweet.\n",
    "* **ct**: saves the size of the training set.\n",
    "* **input_ids**: saves the IDs for each token the algorithm detected. Token IDs are numerical representations of tokens building sequences.\n",
    "* **attention_mask**: As tweets have different sizes, attention_mask signals the token IDs that should be read by the model. It indicates to the model which tokens should be attended to, and which should not.\n",
    "* **token_type_ids** Some models' purpose it to do questions answering. With roBERTa, token type IDs identify which section of the phrase is a question and which is an answer. As this is not our goal, we leave it zero.\n",
    "* **start_token**: Using a one-hot notation, saves the position where the selected fragment starts.\n",
    "* **end_token**: Using a one-hot notation, saves the position where the selected fragment ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 96\n",
    "ct = train_set.shape[0]\n",
    "input_ids = np.ones((ct, max_length), dtype='int32')\n",
    "attention_mask = np.zeros((ct, max_length), dtype='int32')\n",
    "token_type_ids = np.zeros((ct, max_length),dtype='int32')\n",
    "start_tokens = np.zeros((ct, max_length), dtype='int32')\n",
    "end_tokens = np.zeros((ct, max_length), dtype='int32')\n",
    "\n",
    "for i in range(ct):\n",
    "    \n",
    "    #Find where selected text sits inside the tweet\n",
    "    text1 = \" \"+\" \".join(train_set.loc[i, 'text'].split())\n",
    "    text2 = \" \".join(train_set.loc[i, 'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)] = 1\n",
    "    if text1[idx-1] == ' ':\n",
    "        chars[idx-1] = 1\n",
    "        \n",
    "    #Encode the text and find the selected_text offset, for the\n",
    "    #encoded vector might not have the same length as the text vector\n",
    "    enc = tokenizer.encode(text1)\n",
    "    offsets = []\n",
    "    idx = 0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx, idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    #Find the ids of the selected_text in the tokenized text\n",
    "    tokens = []\n",
    "    for k, (a, b) in enumerate(offsets):\n",
    "        s = np.sum(chars[a:b])\n",
    "        if s > 0:\n",
    "            tokens.append(k)\n",
    "    \n",
    "    #After precessing, fill the vectors\n",
    "    sent_token = sentiment_id[train_set.loc[i, 'sentiment']]\n",
    "    input_ids[i, :len(enc.ids)+5] = [0] + enc.ids + [2,2] + [sent_token] + [2]\n",
    "    attention_mask[i, :len(enc.ids)+5] = 1\n",
    "    \n",
    "    if len(tokens) > 0:\n",
    "        start_tokens[i, tokens[0]+1]=1\n",
    "        end_tokens[i, tokens[-1]+1] = 1"
   ]
  },
  {
   "source": [
    "### Loading testing set\n",
    "\n",
    "We must enconde our testing set the same way we tokenized our training set. Variables here have analogous names to those of the last case."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv(path+'/test.csv').fillna('')\n",
    "\n",
    "ct = test_set.shape[0]\n",
    "input_ids_test = np.ones((ct, max_length), dtype='int32')\n",
    "attention_mask_test = np.zeros((ct, max_length), dtype='int32')\n",
    "token_type_ids_test = np.zeros((ct, max_length),dtype='int32')\n",
    "\n",
    "#We do not need to find the selected text for the testing set for\n",
    "# the algorithm will detect and it will be compared with the fragment\n",
    "# on the csv file\n",
    "for i in range(ct):\n",
    "    \n",
    "    text1 = \" \" + \" \".join(test_set.loc[i, 'text'].split())\n",
    "    enc = tokenizer.encode(text1)\n",
    "    sentiment_token = sentiment_id[test_set.loc[i, 'sentiment']]\n",
    "    input_ids_test[i, :len(enc.ids)+5] = [0] + enc.ids + [2,2] + [sentiment_token] + [2]\n",
    "    attention_mask_test[i, :len(enc.ids)+5] = 1"
   ]
  },
  {
   "source": [
    "### Build roBERTa Model\n",
    "\n",
    "As we want to use a pretrained roBERTa base model, we will add custom layers to it in order to make our model appropriated to our problem.\n",
    "Hence, the first tokens are input into bert_model and its output is x\\[0\\] as below. Also is worth mentioning that the previous output has a shape in a form of\n",
    "(batch_size, MAX_LEN, 768). Next, we drop out randomly sets input with frequency rate between 0% and 10%, in order to avoid overfitting.\n",
    "Then, we use a 1D convolutional layer three times with 128, 64, and 32 filters, respectively, in such a way that for the first two, we use\n",
    "a *LeakyRelU* activation layer. After the third one, we use a regular densely-connected NN layer, where N=1. Then, we use another LeakyRelU activation layer.\n",
    "Finally, we flatten the result and, after that, we apply a *softmax* activation layer to convert a real vector to a vector of categorical probabilities.\n",
    "Hence, the model output x1 for the start tokens indices and x2 for the end tokens indices.\n",
    "\n",
    "After all, we use a *Fine Tuning* technique to optimize the model by using [Adam algorithm](https://keras.io/api/optimizers/adam/) and compile with *categorical_crossentropy* as the loss function.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((max_length,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((max_length,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((max_length,), dtype=tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(path+'/config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(path+'/pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) # dropout randomly sets input units to 0 with a frequency of 10%\n",
    "    x1 = tf.keras.layers.Conv1D(filters=128, kernel_size=2, padding='same')(x1) # it creates kernel that is convolved with the layer input over a single spatial dimension\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1) # it allows a small gradient when the unit is not active\n",
    "    x1 = tf.keras.layers.Conv1D(filters=64, kernel_size=2, padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Conv1D(filters=32, kernel_size=2, padding='same')(x1)\n",
    "    x1 = tf.keras.layers.Dense(units=1)(x1) # just a regular densely-connected NN layer\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1) # it flattens the input and does not affect the batch size\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1) # it converts a real vector to a vector of categorical probabilities\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0])\n",
    "    x2 = tf.keras.layers.Conv1D(filters=128, kernel_size=2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(filters=64, kernel_size=2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(filters=32, kernel_size=2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.Dense(units=1)(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) # Fine tuning\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer) # computes the crossentropy loss between the labels and predictions\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "### Metric\n",
    "\n",
    "We use the jaccard index to compute the model's accuracy.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "source": [
    "### Train roBERTa Model and predict data\n",
    "\n",
    "We apply the Stratified K-Folds technique, where K is equal to 5 for the training phase. Each Fold computes 3 epochs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "oof_start = np.zeros((input_ids.shape[0], max_length))\n",
    "oof_end = np.zeros((input_ids.shape[0], max_length))\n",
    "preds_start = np.zeros((input_ids_test.shape[0], max_length))\n",
    "preds_end = np.zeros((input_ids_test.shape[0], max_length))\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train_set.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "    # callback to save the Keras model or model weights at some frequency \n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "\n",
    "    # trains the model for a fixed number of epochs (iterations on a dataset)\n",
    "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
    "        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n",
    "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
    "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
    "    \n",
    "    # saves the model to Tensorflow SavedModel or a single HDF5 file\n",
    "    print('Loading model...')\n",
    "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    \n",
    "    # out-of-fold prediction\n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    # generates output predictions for the input samples\n",
    "    print('Predicting Test...')\n",
    "    preds = model.predict([input_ids_test,attention_mask_test,token_type_ids_test],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/skf.n_splits\n",
    "    preds_end += preds[1]/skf.n_splits\n",
    "    \n",
    "    # display jaccard index found for each epoch\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        start_index = np.argmax(oof_start[k,])\n",
    "        end_index = np.argmax(oof_end[k,])\n",
    "        if start_index > end_index: \n",
    "            selected_text = train_set.loc[k,'text'] # if the selected text is not found, we use the whole text \n",
    "        else:\n",
    "            text_value = \" \"+\" \".join(train_set.loc[k,'text'].split())\n",
    "            encode_text = tokenizer.encode(text_value)\n",
    "            selected_text = tokenizer.decode(encode_text.ids[a-1:b])\n",
    "        all.append(jaccard(selected_text,train_set.loc[k,'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all)) # display jaccard index for the current fold\n",
    "    print()\n"
   ]
  },
  {
   "source": [
    "### Test sample"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "for k in range(input_ids_test.shape[0]):\n",
    "    start_index = np.argmax(preds_start[k,])\n",
    "    end_index = np.argmax(preds_end[k,])\n",
    "    if start_index > end_index: \n",
    "        selected_text = test_set.loc[k,'text']\n",
    "    else:\n",
    "        text_value = \" \"+\" \".join(test_set.loc[k,'text'].split())\n",
    "        encode_text = tokenizer.encode(text_value)\n",
    "        selected_text = tokenizer.decode(encode_text.ids[start_index-1:end_index])\n",
    "    all.append(selected_text)\n",
    "\n",
    "test_set['selected_text'] = all\n",
    "pd.set_option('max_colwidth', 96)\n",
    "test_set.sample(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
